---
docid: GL-064
title: GL 004 Production Bug Handling
owner: @DocMaintainer
status: Active
created: 2026-01-08
---

﻿# Production Bug Capture & Handling Strategy

**DocID**: `GL-004`
**Owner**: `@SARAH`
**Status**: Active

This document outlines a comprehensive strategy for capturing, handling, and resolving bugs in the production environment. The goal is to minimize user impact, improve system stability, and create a robust feedback loop for continuous improvement.

## 1. Guiding Principles

- **Automate Capture**: Errors should be captured automatically, not manually reported by users whenever possible.
- **Full-Stack Visibility**: Capture errors across the entire stack (frontend, backend, infrastructure).
- **Rich Context**: Every error report must include enough context to be actionable.
- **Proactive Alerting**: Critical errors should trigger immediate alerts to the responsible team.
- **Data-Driven Prioritization**: Use frequency, user impact, and severity to prioritize fixes.
- **Learn and Prevent**: Every bug is an opportunity to improve our code, tests, and processes.

## 2. Error Capturing Strategy

### Frontend (Vue.js)

- **Implementation**: Use the existing `errorLogging.ts` service.
- **What's Captured**:
    - Unhandled Vue component errors (`app.config.errorHandler`).
    - Unhandled global JavaScript errors (`window.onerror`).
    - Unhandled promise rejections (`unhandledrejection` event).
    - Network request errors (via Axios interceptor in `client.ts`).
    - Manually captured errors using `useErrorLogging()`.

### Backend (.NET)

- **Implementation**:
    - **Wolverine**: Use Wolverine's built-in error handling and dead-letter queue capabilities for messaging.
    - **ASP.NET Core**: Implement a global exception handler middleware.
    - **Serilog**: Use as the standard structured logging provider.
    - **OpenTelemetry**: Distributed tracing and metrics (see Section 6).
- **What's Captured**:
    - Unhandled API exceptions.
    - Database connection or query failures.
    - Failed background jobs or message handlers.
    - Validation errors (logged at a lower severity).
    - **Long-running requests** (see Section 6).

### Infrastructure (Docker/Kubernetes)

- **Implementation**: Use a log aggregation tool like the **EFK stack (Elasticsearch, Fluentd, Kibana)**.
- **What's Captured**:
    - Container logs (stdout/stderr) from all services.
    - Kubernetes events.
    - Ingress controller logs for HTTP status codes (5xx, 4xx).
    - Aspire Dashboard logs (in development/staging).

## 3. Error Reporting & Aggregation

- **Centralized Logging**: All logs (frontend, backend, infra) will be sent to **Elasticsearch**. This provides a single place to search, analyze, and correlate events.
- **Error Grouping**: Use the `fingerprint` generated by the frontend `errorLogging.ts` service to group similar errors in Kibana. For the backend, group by exception type and location.
- **Dashboarding**: Create a dedicated **"Production Health" dashboard** in Kibana to visualize:
    - Error rates over time (per service).
    - Top 10 new errors this week.
    - Errors by user impact (number of affected users).
    - API endpoint error rates.

## 4. Context Enrichment

To make errors actionable, the following context must be included:

- **Frontend**:
    - User ID and Tenant ID.
    - Route path and name.
    - Component name and lifecycle hook.
    - User agent, URL.
    - Sanitized stack trace.
- **Backend**:
    - Request trace ID (to correlate frontend and backend logs).
    - Authenticated User ID and Tenant ID.
    - Request payload (sanitized to remove PII).
    - Exception type, message, and full stack trace.
- **Infrastructure**:
    - Container/Pod name.
    - Service name.
    - Timestamp.

## 5. Alerting

- **Tool**: Use **ElastAlert** or Kibana's built-in alerting features.
- **Alerting Rules**:
    - **P0 (Critical)**: Immediate page/alert to on-call `@DevOps` and `@TechLead`.
        - Spike in 5xx server errors (>5% of requests).
        - Critical service is down (health check failing).
        - Fatal frontend error affecting >10% of users.
    - **P1 (High)**: High-priority ticket created in GitHub Issues, notification in team chat.
        - New, unhandled exception seen >5 times.
        - Spike in 4xx client errors (potential breaking API change).
        - A specific feature's error rate is high.
    - **P2 (Medium)**: Daily digest email/report.
        - Low-frequency handled exceptions.

## 6. Triage & Prioritization Workflow

1.  **New Error Alert**: A P1 alert creates a new GitHub Issue with the `bug` and `production` labels, assigned to `@TechLead`.
2.  **Initial Triage (@TechLead)**:
    - Reproduce the issue if possible.
    - Assess user impact (one user, subset, all users?).
    - Assess business impact (blocks core functionality, cosmetic, etc.).
    - Assign a priority label (`P0` to `P3`).
    - Assign the issue to the relevant team lead (`@Backend`, `@Frontend`).
3.  **Prioritization**:
    - `P0` bugs are "stop everything" and require an immediate hotfix.
    - `P1` bugs are scheduled for the current or next sprint.
    - `P2` bugs are added to the backlog.
    - `P3` bugs are noted but may not be fixed if low-impact.

## 7. Debugging & Resolution

- **Use Kibana**: Use the `trace ID` to view the full request lifecycle from frontend to backend.
- **Use Aspire Dashboard**: In development, the Aspire Dashboard provides a rich view of all traces, logs, and metrics.
- **Source Maps**: Ensure frontend source maps are uploaded to a secure location (or accessible by Kibana) to de-minify stack traces.
- **Hotfixes**: For `P0` bugs, a hotfix branch is created from `main`. The fix is deployed directly to production after a minimal, targeted review and testing. The fix is then merged back into `develop`.
- **Standard Fixes**: For `P1`/`P2` bugs, the fix follows the standard development workflow.
- **Add Regression Tests**: Every bug fix **must** be accompanied by a new unit or E2E test that would have caught the bug.

## 8. User Feedback Loop

- **"Report an Issue" Button**: Add a simple feedback button in the app UI.
- **Mechanism**: This button can pre-fill a GitHub issue, or simply open a `mailto:` link. For a more advanced solution, it could use a tool like Atlassian's Jira Service Management or Freshdesk.
- **Link to Errors**: If a user is experiencing an issue, we can ask them for the `Error ID` shown in the console to quickly look up the full context in Kibana.

## 9. Post-Mortem & Prevention

- **Root Cause Analysis (RCA)**: For all `P0` and `P1` bugs, a brief RCA document is created in `.ai/knowledgebase/rca/`.
- **Template**:
    - **Summary**: What happened?
    - **Timeline**: Key events.
    - **Root Cause**: Why did it happen?
    - **Resolution**: What was the fix?
    - **Lessons Learned**: What can we do to prevent this class of bug in the future? (e.g., "Add more robust validation to X service", "Improve E2E test coverage for Y feature").

---

## Appendix A: OpenTelemetry & Performance Monitoring

### Overview

OpenTelemetry provides distributed tracing and metrics across all backend services. This enables:

- **Request tracing**: Follow a single request across multiple services.
- **Performance analysis**: Identify slow endpoints and bottlenecks.
- **Long-running request detection**: Automatically log warnings for requests exceeding thresholds.

### Configuration

OpenTelemetry is configured in `ServiceDefaults/Extensions.cs` and automatically applied to all services that use `AddServiceDefaults()`.

**Instrumentation includes:**
- ASP.NET Core requests (traces + metrics)
- HTTP client requests (outbound calls)
- Runtime metrics (GC, threads, etc.)

**Exporters:**
- **Console**: Enabled in development for local debugging.
- **OTLP**: Enabled when `OTEL_EXPORTER_OTLP_ENDPOINT` is set (e.g., Aspire Dashboard, Jaeger, or Grafana Tempo).

### Long-Running Request Detection

The `LongRunningRequestMiddleware` automatically logs warnings for slow requests:

| Threshold | Log Level | Action |
|-----------|-----------|--------|
| ≥ 3 seconds | Warning | Logged with trace ID for investigation |
| ≥ 10 seconds | Error | Critical alert, immediate investigation |

**Example log output:**
```
WARNING: Slow request detected. Method: GET, Path: /api/catalog/products, Duration: 4532.45ms, StatusCode: 200, TraceId: abc123
```

### How to Use in Production

1. **Set environment variables**:
   ```bash
   OTEL_SERVICE_NAME=B2X-admin
   OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4317
   ```

2. **Query in Jaeger/Grafana**:
   - Filter by `http.server.request.duration > 3s`
   - Group by `http.route` to find slow endpoints

3. **Create alerts** in your monitoring system for:
   - P99 latency > 2 seconds
   - Error rate > 1%
   - Long-running requests (> 10s)

### Key Metrics to Monitor

| Metric | Description | Alert Threshold |
|--------|-------------|-----------------|
| `http.server.request.duration` | Request latency histogram | P99 > 2s |
| `http.server.active_requests` | Current concurrent requests | > 100 |
| `http.client.request.duration` | Outbound HTTP call latency | P99 > 5s |
| `process.runtime.dotnet.gc.collections.count` | GC pressure | Spike detection |
- **Lessons Learned**: Key takeaways are added to `.ai/knowledgebase/lessons.md` to inform future development.
